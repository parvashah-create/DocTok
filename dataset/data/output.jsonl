{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is LangGraph used for?"}, {"role": "assistant", "content": "LangGraph \n\n  Building language agents as graphs  \nNote\nLooking for the JS version? Click here (JS docs).\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the core benefits of using LangGraph for building stateful, multi-actor applications with LLMs?"}, {"role": "assistant", "content": "Overview LangGraph is a library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows. Compared to other LLM frameworks, it offers these core benefits: cycles, controllability, and persistence. LangGraph allows you to define flows that involve cycles, essential for most agentic architectures, differentiating it from DAG-based solutions. As a very low-level framework, it provides fine-grained control over both the flow and state of your application, crucial for creating reliable agents. Additionally, LangGraph includes built-in persistence, enabling advanced human-in-the-loop and memory features. LangGraph is inspired by Pregel and Apache Beam. The public interface draws inspiration from NetworkX. LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key features of LangGraph?"}, {"role": "assistant", "content": "Key Features \nCycles and Branching: Implement loops and conditionals in your apps.\nPersistence: Automatically save state after each step in the graph. Pause and resume the graph execution at any point to support error recovery, human-in-the-loop workflows, time travel and more.\nHuman-in-the-Loop: Interrupt graph execution to approve or edit next action planned by the agent.\nStreaming Support: Stream outputs as they are produced by each node (including token streaming).\nIntegration with LangChain: LangGraph integrates seamlessly with LangChain and LangSmith (but does not require them).\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What command is used to install or update langgraph?"}, {"role": "assistant", "content": "Installation pip install -U langgraph\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the weather in San Francisco according to the LangGraph example?"}, {"role": "assistant", "content": "Example One of the central concepts of LangGraph is state. Each graph execution creates a state that is passed between nodes in the graph as they execute, and each node updates this internal state with its return value after it executes. The way that the graph updates its internal state is defined by either the type of graph chosen or a custom function. Let's take a look at a simple example of an agent that can use a search tool. pip install langchain-anthropic\n export ANTHROPIC_API_KEY=sk-...\n Optionally, we can set up LangSmith for best-in-class observability. export LANGSMITH_TRACING=true\nexport LANGSMITH_API_KEY=lsv2_sk_...\n from typing import Annotated, Literal, TypedDict\n\nfrom langchain_core.messages import HumanMessage\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_core.tools import tool\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import END, StateGraph, MessagesState\nfrom langgraph.prebuilt import ToolNode\n\n# Define the tools for the agent to use\n@tool\ndef search(query: str):\n    \"\"\"Call to surf the web.\"\"\"\n    # This is a placeholder, but don't tell the LLM that...\n    if \"sf\" in query.lower() or \"san francisco\" in query.lower():\n        return \"It's 60 degrees and foggy.\"\n    return \"It's 90 degrees and sunny.\"\n\ntools = [search]\n\ntool_node = ToolNode(tools)\n\nmodel = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0).bind_tools(tools)\n\n# Define the function that determines whether to continue or not\ndef should_continue(state: MessagesState) -> Literal[\"tools\", END]:\n    messages = state['messages']\n    last_message = messages[-1]\n    # If the LLM makes a tool call, then we route to the \"tools\" node\n    if last_message.tool_calls:\n        return \"tools\"\n    # Otherwise, we stop (reply to the user)\n    return END\n\n# Define the function that calls the model\ndef call_model(state: MessagesState):\n    messages = state['messages']\n    response = model.invoke(messages)\n    # We return a list, because this will get added to the existing list\n    return {\"messages\": [response]}\n\n# Define a new graph\nworkflow = StateGraph(MessagesState)\n\n# Define the two nodes we will cycle between\nworkflow.add_node(\"agent\", call_model)\nworkflow.add_node(\"tools\", tool_node)\n\n# Set the entrypoint as `agent`\n# This means that this node is the first one called\nworkflow.set_entry_point(\"agent\")\n\n# We now add a conditional edge\nworkflow.add_conditional_edges(\n    # First, we define the start node. We use `agent`.\n    # This means these are the edges taken after the `agent` node is called.\n    \"agent\",\n    # Next, we pass in the function that will determine which node is called next.\n    should_continue,\n)\n\n# We now add a normal edge from `tools` to `agent`.\n# This means that after `tools` is called, `agent` node is called next.\nworkflow.add_edge(\"tools\", 'agent')\n\n# Initialize memory to persist state between graph runs\ncheckpointer = MemorySaver()\n\n# Finally, we compile it!\n# This compiles it into a LangChain Runnable,\n# meaning you can use it as you would any other runnable.\n# Note that we're (optionally) passing the memory when compiling the graph\napp = workflow.compile(checkpointer=checkpointer)\n\n# Use the Runnable\nfinal_state = app.invoke(\n    {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]},\n    config={\"configurable\": {\"thread_id\": 42}}\n)\nfinal_state[\"messages\"][-1].content\n \"Based on the search results, I can tell you that the current weather in San Francisco is:\\n\\nTemperature: 60 degrees Fahrenheit\\nConditions: Foggy\\n\\nSan Francisco is known for its microclimates and frequent fog, especially during the summer months. The temperature of 60F (about 15.5C) is quite typical for the city, which tends to have mild temperatures year-round. The fog, often referred to as \"Karl the Fog\" by locals, is a characteristic feature of San Francisco\\'s weather, particularly in the mornings and evenings.\\n\\nIs there anything else you\\'d like to know about the weather in San Francisco or any other location?\"\n Now when we pass the same \"thread_id\", the conversation context is retained via the saved state (i.e. stored list of messages) final_state = app.invoke(\n    {\"messages\": [HumanMessage(content=\"what about ny\")]},\n    config={\"configurable\": {\"thread_id\": 42}}\n)\nfinal_state[\"messages\"][-1].content\n \"Based on the search results, I can tell you that the current weather in New York City is:\\n\\nTemperature: 90 degrees Fahrenheit (approximately 32.2 degrees Celsius)\\nConditions: Sunny\\n\\nThis weather is quite different from what we just saw in San Francisco. New York is experiencing much warmer temperatures right now. Here are a few points to note:\\n\\n1. The temperature of 90F is quite hot, typical of summer weather in New York City.\\n2. The sunny conditions suggest clear skies, which is great for outdoor activities but also means it might feel even hotter due to direct sunlight.\\n3. This kind of weather in New York often comes with high humidity, which can make it feel even warmer than the actual temperature suggests.\\n\\nIt's interesting to see the stark contrast between San Francisco's mild, foggy weather and New York's hot, sunny conditions. This difference illustrates how varied weather can be across different parts of the United States, even on the same day.\\n\\nIs there anything else you'd like to know about the weather in New York or any other location?\"\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the main steps involved in initializing and executing a LangChain graph using ChatAnthropic as the LLM?"}, {"role": "assistant", "content": "Step-by-step Breakdown \n\nInitialize the model and tools.\n\nwe use ChatAnthropic as our LLM. NOTE: we need make sure the model knows that it has these tools available to call. We can do this by converting the LangChain tools into the format for OpenAI tool calling using the .bind_tools() method.\nwe define the tools we want to use - a search tool in our case. It is really easy to create your own tools - see documentation here on how to do that here.\n   \n\nInitialize graph with state.\n\nwe initialize graph (StateGraph) by passing state schema (in our case MessagesState)\nMessagesState is a prebuilt state schema that has one attribute -- a list of LangChain Message objects, as well as logic for merging the updates from each node into the state\n   \n\nDefine graph nodes.\nThere are two main nodes we need:\n\nThe agent node: responsible for deciding what (if any) actions to take.\nThe tools node that invokes tools: if the agent decides to take an action, this node will then execute that action.\n   \n\nDefine entry point and graph edges.\nFirst, we need to set the entry point for graph execution - agent node.\nThen we define one normal and one conditional edge. Conditional edge means that the destination depends on the contents of the graph's state (MessageState). In our case, the destination is not known until the agent (LLM) decides.\n\nConditional edge: after the agent is called, we should either:\na. Run tools if the agent said to take an action, OR\nb. Finish (respond to the user) if the agent did not ask to run tools\n\nNormal edge: after the tools are invoked, the graph should always return to the agent to decide what to do next\n   \n\nCompile the graph.\n\nWhen we compile the graph, we turn it into a LangChain Runnable, which automatically enables calling .invoke(), .stream() and .batch() with your inputs\nWe can also optionally pass checkpointer object for persisting state between graph runs, and enabling memory, human-in-the-loop workflows, time travel and more. In our case we use MemorySaver - a simple in-memory checkpointer\n\nExecute the graph.\n\nLangGraph adds the input message to the internal state, then passes the state to the entrypoint node, \"agent\".\nThe \"agent\" node executes, invoking the chat model.\nThe chat model returns an AIMessage. LangGraph adds this to the state.\n\nGraph cycles the following steps until there are no more tool_calls on AIMessage:\n\nIf AIMessage has tool_calls, \"tools\" node executes\nThe \"agent\" node executes again and returns AIMessage\n\nExecution progresses to the special END value and outputs the final state.\nAnd as a result, we get a list of all our chat messages as output.\n   \n\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What types of resources are available in the LangGraph documentation?"}, {"role": "assistant", "content": "Documentation \nTutorials: Learn to build with LangGraph through guided examples.\nHow-to Guides: Accomplish specific things within LangGraph, from streaming, to adding memory & persistence, to common design patterns (branching, subgraphs, etc.), these are the place to go if you want to copy and run a specific code snippet.\nConceptual Guides: In-depth explanations of the key concepts and principles behind LangGraph, such as nodes, edges, state and more.\nAPI Reference: Review important classes and methods, simple examples of how to use the graph and checkpointing APIs, higher-level prebuilt components and more.\nCloud (beta): With one click, deploy LangGraph applications to LangGraph Cloud.\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can I find more information on how to contribute?"}, {"role": "assistant", "content": "Contributing For more information on how to contribute, see here."}]}
